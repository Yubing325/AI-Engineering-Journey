# 🗓️ Daily Log – 2025-10-21

**Focus:** Embeddings Deeper Look

## 🧠 What I Learned
- 3 Types of embedding
    - Word2Vec: Fast and simple but only has one vector
    - GloVe: Performs well on similarity and analogy tasks but Fixed-size vocabulary and static vectors
    - Transformer-based Embeddings (e.g., BERT, GPT, Sentence-BERT)
        Pros:
            ✅ Context-aware (e.g., “river bank” ≠ “bank account”)
            ✅ State-of-the-art for search, clustering, semantic similarity
            ✅ Works for words, sentences, or documents
        Cons:
            ❌ Heavier computational cost
            ❌ Requires fine-tuning for best results



## 💻 Snippet / Command
```py
from sentence_transformers import SentenceTransformer

# 1. Load a pre-trained model
# This model is fast and popular for sentence embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

sentences = [
    "The cat sat on the mat.",
    "A dog was chasing a ball."
]

# 2. Call .encode() to get the embeddings
sentence_embeddings = model.encode(sentences)

print(f"\nShape of embeddings: {sentence_embeddings.shape}")
print("(This means 2 sentences, each with a vector of 384 numbers)")

# You can see the vector for the first sentence:
print(f"\nVector for first sentence (first 5 numbers):")
print(sentence_embeddings[0][:5])
output:
Shape of embeddings: (2, 384)
(This means 2 sentences, each with a vector of 384 numbers)

Vector for first sentence (first 5 numbers):
[ 0.1302372  -0.01577281 -0.03671669  0.05798639 -0.05979172]
```

⚠️ Speed-Bump / Question
- How to choose appropriate embedding models
- How to fine-tune embedding models?

🔗 Links / References
- https://www.youtube.com/watch?time_continue=48&v=b6aMdmbjCvA&embeds_referring_euri=https%3A%2F%2Fgemini.google.com%2F&embeds_referring_origin=https%3A%2F%2Fgemini.google.com&source_ve_path=Mjg2NjY
- https://www.youtube.com/watch?v=hVM8qGRTaOA
