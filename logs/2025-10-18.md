# üóìÔ∏è Daily Log ‚Äì 2025-10-18

**Focus:** LLMs Categories/Transformer Types

## üß† What I Learned
- Modern LLMs can be categorized in three main ways
    - By Transformer Architecture Type (encoder / decoder / encoder-decoder)
    - By Functional Purpose (understanding vs generation vs hybrid)
    - By Training Paradigm (pretrained-only, fine-tuned, instruction-tuned, RLHF)
- Transformer Architecture Types
    - Encoder-only = ‚ÄúUnderstand and represent.‚Äù e.g. BERT, RoBERTa, DistilBERT, ALBERT
	- Decoder-only = ‚ÄúPredict and write.‚Äù e.g. GPT-2, GPT-3, GPT-4, LLaMA, Falcon, Mistral
	- Encoder-decoder = ‚ÄúRead and then write.‚Äù.e.g. T5, BART, FLAN-T5, PEGASUS
-  PyPI.Transformers (i.e. HuggingFace Transformers) => Pipeline utitily


## üíª Snippet / Command
```py
from transformers import pipeline

# Encoder-only model (BERT) for understanding
classifier = pipeline("sentiment-analysis", model="bert-base-uncased")
print(classifier("I really enjoy learning NLP with Python!"))

# Decoder-only model (GPT) for generation
generator = pipeline("text-generation", model="gpt2")
print(generator("Once upon a time in the world of NLP,", max_length=30))

outputs:
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cpu
[{'label': 'LABEL_1', 'score': 0.6108905673027039}]
Device set to use cpu
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
[{'generated_text': "Once upon a time in the world of NLP, I have been called by many names.\n\nI am the one who can bring about the transformation of the world of NLP. I am the one that makes the world a better place. I am the one that will end wars and keep human lives alive, that will make society safer, that will make the world a better place. I am the one that will bring about the end of the world of NLP and start a new era of peace. I am the one that will bring about the end of the world of NLP and start an international movement to end the war on drugs. I am the one that will bring about the end of the world of NLP and will stop the war on drugs and start a new era of peace. I am the one that will bring about the end of the world of NLP and will stop the war on drugs and start an international movement to end the war on poverty.\n\nI am the one that will bring about the end of the world of NLP and start an international movement to end the war on poverty and end the war on drugs. I'm the one that will bring about the end of the world of NLP and end the war on drug. I'm the one who will bring about the end of"}]
```

‚ö†Ô∏è Speed-Bump / Question
- how pipeline works exactly
- why the outputs above doesn't indicate classifier result?

üîó Links / References
	
