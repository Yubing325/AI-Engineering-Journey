# 🗓️ Daily Log – YYYY-MM-DD

**Focus:** Deploy a simplest embedding model

## 🧠 What I Learned
- LocalHost/Self-host in cloud/ Open Services
- Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art embedding and reranker models. It can be used to compute embeddings using Sentence Transformer models (quickstart), to calculate similarity scores using Cross-Encoder (a.k.a. reranker) models (quickstart), or to generate sparse embeddings using Sparse Encoder models (quickstart). This unlocks a wide range of applications, including semantic search, semantic textual similarity, and paraphrase mining.

## 💻 Snippet / Command
Below code snippets from: https://www.sbert.net
```py
from sentence_transformers import SentenceTransformer

# 1. Load a pretrained Sentence Transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# The sentences to encode
sentences = [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium.",
]

# 2. Calculate embeddings by calling model.encode()
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# 3. Calculate the embedding similarities
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.6660, 0.1046],
#         [0.6660, 1.0000, 0.1411],
#         [0.1046, 0.1411, 1.0000]])
```

⚠️ Speed-Bump / Question
- Something unclear or worth exploring tomorrow

🔗 Links / References  
the paper of Sentence-BERT:
- https://arxiv.org/abs/1908.10084  
ONNX:
- https://onnxruntime.ai/
