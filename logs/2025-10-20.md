# 🗓️ Daily Log – 2025-10-20

**Focus:** Embeddings

## 🧠 What I Learned
- Embeddings vs Token IDs (in Tokenization)
    - Embedding are vectors that means someting while tokenIDs do not have meanings.


## 💻 Snippet / Command
```py
# Our raw text
text = "the cat sat"

# --- 1. Tokenization (Splitting) ---
tokens = text.lower().split()
print(f"Tokens: {tokens}")

# --- 2. Numericalization (Getting Token IDs) ---
# In a real model, this vocabulary has 50,000+ items.
# We'll make a tiny one.
vocabulary = {
    "the": 101, 
    "cat": 500, 
    "sat": 720,
    "dog": 501  # Another word in our vocab
}

# Convert our string tokens into number IDs
token_ids = [vocabulary[token] for token in tokens]
print(f"Token IDs: {token_ids}")

# --- 3. Embedding (The Lookup) ---
# This is our "Embedding Matrix" or "Lookup Table".
# The key is the Token ID.
# The value is the embedding vector.
# (We'll use tiny 4-number vectors for this example)
embedding_table = {
    101: [0.1, 0.4, -0.2, 0.9],  # Vector for "the"
    500: [0.8, -0.1, 0.5, 0.3],  # Vector for "cat"
    501: [0.7, -0.2, 0.4, 0.2],  # Vector for "dog"
    720: [-0.5, 0.3, 0.8, 0.1]   # Vector for "sat"
}

# Look up the vector for each of our token IDs
embeddings = [embedding_table[id] for id in token_ids]

print(f"Embeddings (Vectors): {embeddings}")

print("\n--- Summary ---")
for i in range(len(tokens)):
  print(f"Token: '{tokens[i]}'  -> ID: {token_ids[i]}  -> Embedding: {embeddings[i]}")

outputs:
Tokens: ['the', 'cat', 'sat']
Token IDs: [101, 500, 720]
Embeddings (Vectors): [[0.1, 0.4, -0.2, 0.9], [0.8, -0.1, 0.5, 0.3], [-0.5, 0.3, 0.8, 0.1]]

--- Summary ---
Token: 'the'  -> ID: 101  -> Embedding: [0.1, 0.4, -0.2, 0.9]
Token: 'cat'  -> ID: 500  -> Embedding: [0.8, -0.1, 0.5, 0.3]
Token: 'sat'  -> ID: 720  -> Embedding: [-0.5, 0.3, 0.8, 0.1]
```
⚠️ Speed-Bump / Question
- 

🔗 Links / References
- https://www.youtube.com/watch?v=izbifbq3-eI&t=5s
