# üóìÔ∏è Daily Log ‚Äì YYYY-MM-DD

**Focus:** Embeddings

## üß† What I Learned
- Smaller vectors (lower dimensions) are more efficient to keep in memory or to process, while bigger vectors (higher dimensions) can capture intricate relationships, 
but are prone to overfitting
- Dense vectors contain mostly non-zero values and capture nuanced semantic meaning, while sparse vectors have mostly zero values and are efficient for capturing keyword-based information
- Q:How to choose appropriate embedding models?\
A: Refer Benchmarks such as MTEB
- STS: Semantic textual similarityÔºö measures how similar two pieces of text are in meaning ‚Äî not just in wording.„ÄÅ
the relationship with emebedding: embeddings are often used to perform STS.
- Why embedding is fast: The system finds the closest vectors in the database using Approximate Nearest Neighbor (ANN) algorithms.
It converts to a math problem in high-dimensional space, not text parsing.

## üíª Snippet / Command
```py
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, InputExample, evaluation

# 1Ô∏è‚É£ Load model
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2Ô∏è‚É£ Load STS Benchmark test split from Hugging Face
dataset = load_dataset("stsb_multi_mt", name="en", split="test")

# 3Ô∏è‚É£ Convert each pair into InputExample
samples = [
    InputExample(texts=[row["sentence1"], row["sentence2"]], label=float(row["similarity_score"]) / 5.0)
    for row in dataset
]

# 4Ô∏è‚É£ Build evaluator
evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(samples, name='sts-benchmark-test')

# 5Ô∏è Evaluate
model.evaluate(evaluator)
output:
{'sts-benchmark-test_pearson_cosine': 0.8274064173808451,
 'sts-benchmark-test_spearman_cosine': 0.8203247283076371}

```

‚ö†Ô∏è Speed-Bump / Question
- vector index (e.g. FAISS, Milvus, Pinecone), what are they?

üîó Links / References
- https://www.youtube.com/watch?v=AT5nuQQO96o
